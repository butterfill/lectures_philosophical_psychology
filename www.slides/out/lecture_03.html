<!-- - scripts.add(["/vendor/deck.js/deck.core.js"])--><!-- - scripts.add(["/vendor/deck.js/deck.menu.js"])--><!-- - scripts.add(["/vendor/deck.js/deck.hash.js"])--><!-- - scripts.add(["/vendor/deck.js/deck.notes.js"])--><html xmlns="http://www.w3.org/1999/xhtml" xmlns:svg="http://www.w3.org/2000/svg"><head><!-- (c) copyright 2013 Stephen A. Butterfill--><meta charset="utf-8"/><meta http-equiv="content-type" content="text/html; charset=utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><title>Lecture 03: On the Motor Theory of Speech Perception | Philosophical Psychology</title><meta name="description" content="Slides for a lecture by s.butterfill@warwick.ac.uk"/><meta name="keywords" content="philosophy, psychology, action, joint action, metarepresentation, perception"/><meta name="author" content=""/><meta name="generator" content="DocPad v6.78.4" /><meta name="viewport" content="width=device-width"/><!-- - fonts--><link href="https://fonts.googleapis.com/css?family=Lato:300,400,900,400italic" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=The+Girl+Next+Door" rel="stylesheet" type="text/css"/><!--if lt IE 9script(async src="https://html5shim.googlecode.com/svn/trunk/html5.js")
--><link  rel="stylesheet" href="/vendor/boilerplate.min.css" /><link  rel="stylesheet" href="/vendor/normalize.min.css" /><link  rel="stylesheet" href="/vendor/deck.js/deck.all.min.css" /><link  rel="stylesheet" href="/vendor/960_12_col_custom.min.css" /><link  rel="stylesheet" href="/styles/steve_deck_style.css" /><link  rel="stylesheet" href="/vendor/questionmark.js/question.mark.css" /></head><body><script defer="defer"  src="/vendor/jquery.min.js"></script><script defer="defer"  src="/vendor/jquery-svgfix.js"></script><script defer="defer"  src="/vendor/jquery.crSplineBkg.js"></script><script defer="defer"  src="/vendor/velocity.min.js"></script><script defer="defer"  src="/vendor/marked.min.js"></script><script defer="defer"  src="/vendor/modernizr.custom.01932.js"></script><script defer="defer"  src="/vendor/deck.js/deck.velocity.js"></script><script defer="defer"  src="/vendor/deck.js/deck.all.min.js"></script><script defer="defer"  src="/vendor/jquery.jsPlumb-1.3.16-all-min.js"></script><script defer="defer"  src="/scripts/script.js"></script><script defer="defer"  src="/vendor/questionmark.js/question.mark.min.js"></script><div id="helpUnderlay" class="help-underlay"><div id="helpModal" class="help-modal"><h1>Keyboard Shortcuts<kbd class="help-key"><span>?</span></kbd></h1><div id="helpClose" class="help-close">×</div><!-- .help-close--><div id="helpModalContent" class="help-modal-content"><div id="helpListWrap" class="help-list-wrap"><ul class="help-list"><li class="help-key-unit"><kbd class="help-key"><span>→</span></kbd><span class="help-key-def">Next step</span></li><li class="help-key-unit"><kbd class="help-key"><span>←</span></kbd><span class="help-key-def">Previous step</span></li><li class="help-key-unit"><kbd class="help-key"><span>↓</span></kbd><span class="help-key-def">Skip this slide</span></li><li class="help-key-unit"><kbd class="help-key"><span>↑</span></kbd><span class="help-key-def">Previous slide</span></li><li class="help-key-unit"><kbd class="help-key"><span>m</span></kbd><span class="help-key-def">Show slide thumbnails</span></li><li class="help-key-unit"><kbd class="help-key"><span>n</span></kbd><span class="help-key-def">Show notes</span></li><li class="help-key-unit"><kbd class="help-key"><span>h</span></kbd><span class="help-key-def">Show handout latex source</span></li><li class="help-key-unit"><kbd class="help-key"><span>N</span></kbd><span class="help-key-def">Show talk notes latex source</span></li></ul><!-- .help-list--></div><!-- .help-list-wrap--></div><!-- .help-modal-content--></div><!-- .help-modal--></div><!-- .help-underlay-->


<div class="deck-notes"><div class="deck-notes-container"></div></div><div class="deck-handout"><div class="deck-handout-container"></div></div><div class="deck-container"><section id="instructions" class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div class="middle"><p class="center">Click here and press the right key for the next slide (or swipe left)</p></div></div></div></div></section><section class="slide"><!-- .notes You won't believe how many people emailed me to say the slides don't work.--><div class="words"><div class="container_12"><div class="grid_12"><p>also ...</p><p>Press the left key to go backwards (or swipe right)</p><p>Press n to toggle whether notes are shown (or add '?notes' to the url before the #)</p><p>Press m or double tap to slide thumbnails (menu)</p><p>Press ? at any time to show the keyboard shortcuts</p></div></div></div></section>

<!-- param @cls and param @options should be objects-->



<div class="notes notes-header-tex">\title {Philosophical Psychology \\ Lecture 03: On the Motor Theory of Speech Perception}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\maketitle</div><div id="title-slide" class="slide"><img src="/img/bkg/joint_action01/AAaaDSC_AA_4400.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div class="fade-in"><div style="position:relative; top:375px" class="title-block"><div class="title-container"><h1 style="line-height:45pt;" class="title1 fade-in">Lecture 03: On the Motor Theory of Speech Perception</h1><h3 class="email fade-in">s.butterfill@warwick.ac.uk</h3></div></div></div><div class="handout">\def \ititle {Lecture 03: On the Motor Theory of Speech Perception}</div><div class="handout">\begin{center}</div><div class="handout">{\Large</div><div class="handout">\textbf{\ititle}</div><div class="handout">}</div><div class="handout">&nbsp;</div><div class="handout">\iemail %<s.butterfill@warwick.ac.uk></div><div class="handout">\end{center}</div></div><div class="handout">&nbsp;</div><div class="handout">\section{A Question about Experiences of (Speech) Actions}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{A Question about Experiences of (Speech) Actions}</div><div class="notes">What do we experience when we encounter others’ actions?
One hypothesis (the Indirect Hypothesis) says that such experiences are all experiences of bodily
configurations, of joint displacements and of effects characteristic of particular actions.
Another hypothesis (the Direct Hypothesis) says that 
in observing an action we sometimes experience not only bodily configurations and joint displacements
and their sensory effects but also the action as directed to a particular outcome.</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">What do we experience when we encounter others’ <span class="speech hide">speech </span><span> actions?</span></p><div class="slide"><p class="em-above center">Indirect Hypothesis <span class="direct-h hide">vs Direct Hypothesis</span></p><div class="slide"><div data-what=".direct-h" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div></div><div class="slide"><div data-what=".speech" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div data-what=".speech" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".speech" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">I suppose that speech actions are actions, so I will start with those.</div></div></div></div></div></div></section><section id="categorical_perception_speech" class="slide"><img src="/img/bkg/joint_action01/DSC_AB_6649.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="position:relative; top:425px" class="title-block"><div class="title-container"><h2 class="title1">Speech Perception</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{Speech Perception}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{Speech Perception}</div><div class="notes">***</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">articulatory gesture</p><div class="notes">In speaking we produce an overlapping sequence of articulatory gestures, which are motor actions
involving coordinated movements of the lips, tongue, velum and larynx. These gestures are the units
in terms of which we plan utterances (Browman and Goldstein 1992; Goldstein, Fowler, et al. 2003).</div><div class="notes">These are the actions I want to focus on first in thinking about
what we experience when we encounter others’ actions.</div></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/browman_1996_fig1.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Browman & Goldstein 1986, figure 1</p><div class="notes">‘Trajectory of lower lip in [abe] as measured by tracking infra-redLED
placed on subject's lower lip’</div><div class="notes">‘Not every utterance of word transcribed with /b/ will display exactly the trajectory
of Fig. 1.: the trajectory will vary with vowel context, syllable position,
stress, speaking rate and speaker.
We must, therefore, ultimately characterise /b/ as a family of patterns of lip
movement’
\citep[p.~224]{browman:1986_towards}</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Speech and auditory perception involve distinct processes</p></div></div></div></div></section><section class="slide"><img src="/img/duplex_01.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">A schematic spectrogram for a synthetic sound which is normally perceived as [ra]. The horizontal
axis represents time, the vertical frequency.</div><div class="notes">A schematic spectrogram for [la].</div></div></div></div></section><section class="slide"><img src="/img/duplex_02.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">In the middle you see the \emph{base}, i.e. the part of the spectrogram common to [ra] and [la]. This
is played to one ear</div><div class="notes">Below you see the transitions, , i.e. the parts of the spectrogram that differ between [ra] and [la].
When played in isolation these sound like a chirp. When played at the same time as the base but in
the other ear, subjects hear a chirp and a [ra] or a [la] depending on which transition is played.</div><div class="notes">How do we know that the same stimuli may be processed by different perceptual systems
concurrently—for instance, how do we know that speech and auditory processing are distinct? A
phenomenon called “duplex” perception demonstrates their distinctness occurs in. Artificial
speech-like stimuli for two syllables, [ra] and [la], are generated. The acoustic signals for each
syllable is artificially broken up into two parts, the “base” and “transition” (see Fig. *** below).
The syllables have the same “base” but differ in the “transition”. When the “transition” is played
alone it sounds like a chirp and quite unlike anything we normally hear in speech. Duplex perception
occurs when the base and transition are played together but in separate ears. In this case, subjects
hear both the chirp that they hear when the transition is played in isolated, and the syllable [la]
or [ra]. Which syllable they hear depends on which transition is played, so speech processing must
have combined the base and transition. By contrast, auditory processing must have failed to combine
them because otherwise the chirp would not have been heard. In this case, then, the perception
resulting from the duplex presentation involves simultaneous auditory and speech recognition
processes. This shows that auditory and speech processing are distinct perceptual processes.</div><div class="notes">The duplex case is unusual. We can’t normally hear the chirps we make in speaking because speech
processing inhibits this level of auditory processing. But plainly speech is subject to some auditory
processing for we can hear extra-linguistic qualities of speech; some of these provide cues to
emotional state, gender and class. Perception of these extra-linguistic qualities enables us to
distinguish stimuli within a category. As already mentioned, this is a problem for Repp’s operational
definition. Our ability to discriminate stimuli is the product of both categorical speech processing
and non-categorical auditory processing. If we want to get at the essence of categorical perception
it seems there is no alternative but to appeal to particular perceptual processes rather than
behaviours.</div><div class="notes">Source: \citep{Liberman:1981xk}</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide01.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Here are 12 speech-like sounds.
Acoustically each differs from its neighbours no more than any other does.</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide02.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">They would be labelled differently</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide03.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">And within a label they are relatively hard to disciminate whereas ...</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide04.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Discriminating acoustically no less similar stimuli that are given
different labels is easier (faster and more accurate).</div><div class="notes">This is categorical perception: speed and accuracy maps onto labelling ...</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide06.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Categorical perception of mating calls and perhaps other acoustic signals is widespread in non-human
animals including monkeys, mice and chinchillas (Ehret 1987; Kuhl 1987), and is even found in
cognitively unsophisticated animals such as frogs (Baugh, Akre and Ryan 2008) and crickets
(Wyttenbach, May and Hoy 1996).</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">What are the objects of categorical perception?</p></div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide26.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">the location of the category boundaries changes depending on contextual factors such as the
speaker’s dialect,22 or the rate at which the speaker talks;23 both factors dramatically affect
which sounds are produced.</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide27.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">This means that in two different contexts, different stimuli may result in the same perceptions, and
the same stimulus may result in different perceptions.</div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide23.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide24.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide25.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">co-articulation, the fact that phonic gestures overlap (this is what makes talking fast).</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">What are the objects of categorical perception?</p><div class="notes show em-above"><p class="em-around">1. Speech perception is categorical</p><p class="em-around">  2. The category boundaries correspond (imperfectly but robustly) to differences in articulatory gestures</p><p class="em-around">  3.  The best explanation of (2) involves the hypothesis that the objects of speech perception are articulatory gestures</p></div><div class="notes">\emph{Articulatory Gesture:}
In speaking we produce an overlapping sequence of articulatory gestures, which are motor actions
involving coordinated movements of the lips, tongue, velum and larynx. These gestures are the units
in terms of which we plan utterances (Browman and Goldstein 1992; Goldstein, Fowler, et al. 2003).</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p>The objects of speech perception are articulatory gestures.</p><p class="em-above">... Does this entail that we perceptually experience articulatory gestures?</p><div class="slide"><p class="em-above">‘Describing [Mary’s experience] as being as of a dodecahedron … is … normally intended to describe its introspectable character’</p><p class="grey-text right">(Martin 1992: 762).</p></div><div class="slide"><p class="em-above">Perceptual experiences are reasons for beliefs ...</p></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">An argument for perceptual experience of articulatory gestures.</p></div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide06.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide07.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide08.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide09.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide10.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">The difference in differences ...</div><div class="notes">Here is the argument. Consider two sequences of sensory encounters: (a) a sequence of sensory
encounters with two phonetic events that do not differ with respect to category (both are
realisations of /d/, say), and (b) a sequence of sensory encounters with two phonetic events that do
so differ (one is a realisation of /d/ the other of /g/, say).11 Let the events encountered in the
first sequence differ from each other acoustically in the same way and by the same amount as the
events encountered in the second sequence differ from each other. (That it is possible to find two
such pairs of events follows from the fact that we enjoy categorical perception of speech.) The two
sequences are depicted in Fig. 3. Now:</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="hem-around">(1) The second sequence of sensory encounters, (b), differ from each other more in phenomenal character than the first sequence of sensory encounters, (a), differ from each other.</p><p class="hem-around slide">(2) This difference in differences in phenomenal character is a fact in need of explanation.</p><p class="hem-around slide">(3) The difference cannot be fully explained by appeal only to perceptual experiences as of acoustic features of the stimuli.</p><p class="hem-around slide">(4) The difference can be explained in terms of perceptual experiences as of phonetic properties.</p><p class="hem-around slide">(5) There is no better explanation of the difference.</p><div class="notes">The fourth step in this argument, (4), needs some filling in. How would the thesis that categorical
perception of speech is a form of perceptual experience explain the difference in differences in
phenomenal character? If the thesis is true, the first sequence of sensory encounters, (a), involves
two perceptual experiences as of a single phoneme whereas the second sequence of encounters, (b),
involves perceptual experiences as of different phonemes.12 Let us assume (not very controversially)
that perceptual experiences have phenomenal characters and that which phenomenal character a
perceptual experience has depends in part on what it is as of.13 It follows that differences in what
perceptual experiences are as of can explain differences in the phenomenal characters of those
perceptual experiences. In particular, if it is a fact that (b) involves perceptual experiences as
of different things whereas (a) does not, this could explain why the sensory encounters in (b)
differ in phenomenal character in a way that the sensory encounters in (a) do not.</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">Recall the question ...</div><p class="center">What do we experience when we encounter others’ <span class="speech">speech </span><span> actions?</span></p><p class="em-above center">Indirect Hypothesis vs  <span class="direct-h">Direct Hypothesis</span></p><div class="slide"><div data-what=".direct-h" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".direct-h" data-cls="bkg-invert" class="dv dv-addclass"></div></div><div class="notes">It looks like the Direct Hypothesis wins, or at least that we must reject the 
Indirect Hypothesis.</div><div class="notes">There’s just one little problem ...</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="notes handout show">How could the objects of categorical perception of speech be articulatory gestures?</p><div class="notes">The puzzle here is twofold.
[a] Categorical perception of speech happens raidly, and goal-directed actions
are complex.  How can something so complex be computed so quickly?
[b] How could you recover articulatory gestures from acoustic and visual inputs?</div><div class="slide"><p class="em-above notes handout show">‘Humans [can] understand speech delivered at a rate of 20 to 30 ... phonemes per second’</p><div class="notes handout ctd"> \citep{Devlin:2006qg}</div><p class="right grey-text">Devlin (2006)</p><div class="notes">Before facing this problem directly, I want to think about action more generally ...</div></div></div></div></div></div></section><section id="double_life_motor_representation" class="slide"><img src="/img/bkg/joint_action01/DSC_AB_6862.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="position:relative; top:425px" class="title-block"><div class="title-container"><h2 class="title1">The Double Life of Motor Representation</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{The Double Life of Motor Representation}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{The Double Life of Motor Representation}</div><div class="notes">Motor representations live a kind of double life. Although paradigmatically involved in performing
actions, they also occur when merely observing others act and sometimes influence thoughts about the
goals of observed actions. Further, these influences are content-respecting: what you think about an
action sometimes depends in part on how that action is represented motorically in you.</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">The Double Life of Motor Representation</p><div class="notes">Suppose you are reaching for, grasping, transporting and then placing a pen. Performing even
relatively simple action sequences like this involves satisfying many constraints that cannot
normally be satisfied by explicit practical reasoning, especially if performance is to be rapid and
fluent. Rather, such performances require motor representations.
These representations are paradigmatically involved in preparing, executing and monitoring actions.%
\footnote{%
See \citet{wolpert:1995internal, miall:1996_forward, jeannerod:1998nbo, zhang:2007_planning}.
Note that motor representations sometimes occur in an agent who has prepared an action and is required (as it turns out) not to perform it: although she has prevented herself from acting, motor representations specifying the action persist, perhaps because they are necessary for monitoring whether prevention has succeeded \citep{bonini:2014_ventral}.
}
But they also live a double life. Motor representations concerning a particular type of action are
involved not only in performing an action of that type but also sometimes in observing one. That is,
if you were to observe Ayesha reach for, grasp, transport and then place a pen, motor representations
would occur in you much like those that would also occur in you if it were you---not Ayesha---who was
doing this.</div><div class="notes">Converging evidence for this assertion comes from a variety of methods and measures ...</div></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/fogassi_2005_fig1B.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Fogassi et al 2005, figure 1B</p><div class="notes">Single cell recordings in nonhuman primates show that, for each of several types of action, there are
populations of neurons that discharge both when an action of this type is performed and when one is
observed \citep{pellegrino:1992_understanding, gallese:1996_action,Fogassi:2005nf}.</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/fogassi_2005_fig1C.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Fogassi et al 2005, figure 1B</p><div class="notes">This is the performance data.</div><div class="notes">Note that the neurons are firing before the distinctive part of the action has
occured: that is, the peak is between movement onset and the monkey first touching
the object to be grasped.</div><div class="notes">Now let’s compare 
performance with observation.</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><img src="/img/fogassi_2005_fig5.png" style=""/><p class="source">Fogassi et al 2005, figure 5</p><div class="notes">‘(A) Congruence between the visual and the motor response of a mirror neuron. Unit 169 has a stronger
discharge during grasping to eat than during grasping to place, both when the action is executed and
when it is observed. Conventions as in Fig. 1. (B) Population-averaged responses during motor and
visual tasks (12).’</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="handout notes"><p>‘word listening produces a phoneme specific activation of speech motor centres’ \citep{Fadiga:2002kl}</p><p class="em-above">‘Phonemes that require in production a strong activation of tongue muscles, automatically produce, when heard, an activation of the listener's motor centres controlling tongue muscles.’ \citep{Fadiga:2002kl}</p></div><p>‘word listening produces a phoneme specific activation of speech motor centres’ </p><div class="slide"><p class="em-above">‘Phonemes that require in production a strong activation of tongue muscles, automatically produce, when heard, an activation of the listener's motor centres controlling tongue muscles.’ </p><div class="notes">Good, but this stops short of showing that the motor activations
actually faciliatate speech recognition ...</div></div><p class="right grey-text">Fadiga et al (2002)</p><div class="notes">How did they reach these conclusions?</div><div class="slide em-above"><p>bi<span class="rr">rr</span><span>a / be</span><span class="rr">rr</span><span>o (pseudo-word) / ba</span><span class="ff">ff</span><span>o</span></p><div class="slide"><div data-what=".rr" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".rr" data-cls="bkg-yellow" class="dv dv-addclass"></div><div class="notes">Inovlves tongue</div></div><div class="slide"><div data-what=".ff" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".ff" data-cls="bkg-blue" class="dv dv-addclass"></div><div class="notes">No tongue required</div><div class="notes">Given TMS to motor cortex tp amplify activity.
Prediction: MEP in tongue muscle stronger for ‘rr’ than ‘ff’.</div></div></div></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/fadiga_2002_fig2.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Fadiga et al 2002, figure 2</p></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide1.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Behaviour: interference effects (ovalization)</div></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide2.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide3.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide4.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide5.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><img src="/img/kilner_2003/Slide6.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes show"><p class="center huge-glow">Puzzle 1</p><p style="margin-top:-3em;" class="center below-huge-glow">What are those motor representations doing here?</p></div><div class="notes">But there is also another puzzle ...</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Motor representations in observation facilitate anticipation of others’ actions.</p></div></div></div></div></section><section class="slide"><img src="/img/costantini_2002.png" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source">Costantini et al, 2012</p><div class="notes">‘We recorded proactive eye movements while participants observed an actor grasping small or large
objects. The participants' right hand either freely rested on the table or held with a suitable grip
a large or a small object, respectively. Proactivity of gaze behaviour significantly decreased when
participants observed the actor reaching her target with a grip that was incompatible with respect to
that used by them to hold the object in their own hand.’</div><div class="notes">Follow ups: tie hands; TMS (impair)</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Motor representations in observation facilitate explicit identification of others’ actions.</p></div></div></div></div></section><section class="slide"><img src="/img/casile_giese_fig1.png" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source">Casile & Giese 2006, figure 1</p><div class="notes">Ability to perform actions: the 180 degree swing is standard walk, whereas
225 and 270 degree swings are not standard but can be trained.</div><div class="notes">Visual discrimination task: ‘The visual recognition experiment was based on a forced-choice
paradigm. In each trial, two point-light stimuli consisting of a total of nine dots were presented
successively, at two different positions on the screen ... Participants had to respond whether both
stimuli represented the same gait pattern. Four cycles of each gait pattern were presented, each
cycle lasting for about 1.2 s. The start position within the gait cycle was randomized across
trials.’</div><div class="notes">Then training while BLINDFOLDED.</div><div class="notes">Then visual recognition task again.</div><div class="notes">‘One possible explanation of the observed motor-visual transfer is that the
participants might have picked up the rhythm that characterizes the trained
motor pattern, but not necessarily the details of the learned body
movement. To rule out this possibility, we performed a control experiment
in which the motor training was replaced by purely visual training.’</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/casile_giese_fig4A.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Casile & Giese 2006, figure 4A</p><div class="notes">Visual performance correlates with motor performance after (but not before) training.
(They also found no correlation with 225 degrees, which was not trained.)</div><div class="notes">This effect is perhaps surprising given that your judgement ultimately rests on purely visual
information (this is the point of the lights) whereas nothing could be seen during the training.</div><div class="notes">What explains this difference in judgement before and after training?  Training of this kind typically alters the way things are represented motorically \citep{Calvo-Merino:2006ru}.
\label{expertise_affects_motor}
For this reason, the increase in the probability of making accurate judgements about the goals of others' actions is plausibly a consequence of differences in motor representations in the observer.</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/pazzaglia_fig1bi.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Pazzaglia et al 2007, figure 1B (part)</p><div class="notes">task: hear sound, identify one of four pictures.</div><div class="notes">Twenty-eight left-hemisphere-damaged patients with or without limb and/or buccofacial apraxia and
seven right- hemisphere-damaged patients with no apraxia were asked to match sounds evoking
human-related actions or nonhu- man action sounds with specific visual pictures.</div><div class="notes">‘In the novel sound-picture matching test used in this study, each patient was asked to listen to a
sound and then choose from among four pictures the one corresponding to the heard sound. The sounds
used included limb-related action sounds (LRAS), buccofacial-related action sounds (BRAS)
[ten sounds were transitive, i.e., object related (e.g., inflating a balloon) and ten were
intransitive, i.e., non object related (e.g., coughing). ], and non-human action-related sounds
(NHARS) [e.g. sea waves, breaking]’</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/pazzaglia_fig1bii.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Pazzaglia et al 2007, figure 1B (part)</p></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/pazzaglia_2008_S1.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Pazzaglia et al 2007, Appendix S1 (fragment)</p></div></div></div></section><section class="slide"><img src="/img/pazzaglia_2008_fig2.png" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source">Pazzaglia et al 2007, figure 2</p><div class="notes">Beautiful results!</div><div class="notes">Key: limb-related action sounds (LRAS), buccofacial-related action sounds (BRAS), and non-
human action-related sounds (NHARS)</div><div class="notes">A+ apraxia; AB+ : buccofacial apraxia; AL+ limb apraxia; LBD: left brain damage; RBD: right brain damage</div></div></div></div></section><section class="slide"><img src="/img/dausilio_2009_fig1.png" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source">D'Ausilio et al (2009, figure 1)</p><div class="notes">‘Double TMS pulses were applied just prior to stimuli presentation to selectively prime the cortical activity specifically in the lip (LipM1) or tongue (TongueM1) area’
\citep[p.~381]{dausilio:2009_motor}</div><div class="notes">‘We hypothesized that focal stimulation would facilitate the perception of 
the concordant phonemes ([d] and [t] with TMS to TongueM1), but that 
there would be inhibition of perception of the discordant items 
([b] and [p] in this case). Behavioral effects were measured via reaction 
times (RTs) and error rates.’ \citep[p.~382]{dausilio:2009_motor}</div></div></div></div></section><section class="slide"><img src="/img/dausilio_2009_fig2.png" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source">D'Ausilio et al (2009, figure 1)</p><div class="notes">‘Effect of TMS on RTs show a double dissociation between stimulation 
site (TongueM1 and LipM1) and discrimination performance between class 
of stimuli (dental and labial). The y axis represents the amount of RT 
change induced by the TMS stimulation. Bars depict SEM. Asterisks 
indicate significance (p < 0.05) at the post-hoc (Newman-Keuls) comparison.’ 
\citep{dausilio:2009_motor}</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes show"><p class="huge-glow">Puzzle 1</p><p style="margin-top:-3em;" class="below-huge-glow">What are those motor representations doing here?</p><div class="slide"><p class="right em-above">Motor representations concerning the goals of observed actions sometimes facilitate the identification of goals.</p></div><div class="slide"><p class="center huge-glow">Question</p><p style="margin-top:-3em;" class="center"> How?</p></div></div><div class="notes">Question: How is it that motor representations concerning the goals of observed actions sometimes facilitate identification of goals?</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Speech Perception Reprise</p><div class="notes">Recall two facts mentioned in discucssing speech perception ...</div><div class="slide"><p class="em-above">Speech and auditory perception involve distinct processes.</p><div class="notes">This makes sense given that, as \citet{dausilio:2009_motor} argue,
motor processes facilitate the identification of articulatory gestures.
After all, motor processes and auditory processes are likely distinct.</div></div><div class="slide"><p class="em-above">The objects of speech perception are articulatory gestures.</p><div class="notes">This also makese sense: given that there’s a link between observing an
articulatory gesture and representing it motorically, it seems that the 
motor representation of the articulatory gesture could be what speech
perception aims at.</div><div class="notes">(I’m ignoring a lot of complexity about how, given enough context,
motor impairments can have very little impact on speech perception.
Maybe not all speech perception depends on recovering articulatory gestures,
and maybe there are multiple kinds of process that enable us to comprehend
what someone is saying.)</div></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">Also recall the question</div><p class="notes show">How could the objects of categorical perception of speech be articulatory gestures?</p><div class="notes">The puzzle here is simple.
Categorical perception of speech happens raidly, and goal-directed actions
are complex.  How can something so complex be computed so quickly?</div><div class="notes">Put it another way: how is it that the articulatory gesture is identified?</div><p class="em-above notes handout show">‘Humans [can] understand speech delivered at a rate of 20 to 30 ... phonemes per second’</p><div class="notes handout ctd"> \citep{Devlin:2006qg}</div><p class="right grey-text">Devlin (2006)</p><div class="notes">Nothing we’ve yet covered enables us to answer this question.</div></div></div></div></div></section><section id="teleological_stance" class="slide"><img src="/img/bkg/joint_action01/DSC_AB_7018.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="" class="title-block"><div class="title-container"><h2 class="title1">The Teleological Stance</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{The Teleological Stance}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{The Teleological Stance}</div><div class="notes">The Teleological Stance (Gergeley and Csibra , 1995) provides a computational
theory of pure goal ascription.
Pure goal ascription is the process of identifying goals to which anothers’
actions are directed independently of any knowledge, or beliefs about,
the intentions or other mental states of an agent.</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center huge-glow-180">How?</p></div></div></div></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Motor representations concerning the goals of observed actions sometimes facilitate the identification of goals.</p></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">To illustrate that there’s really a question here (How is it that motor representations concerning
the goals of observed actions sometimes influence thoughts about them?), consider this about MT:</div><p class="handout notes show">‘it is not clear how these [production-perception] links or MNs [mirror neurons] would solve the
problems of mapping variable acoustics to linguistic representations, which first motivated MT [the
motor theory of speech perception].’</p><div class="notes handout ctd">\citep[p.~205]{holt:2014_alluring}</div><p class="right grey-text">Holt and Lotto 2014, p. 205</p></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">To solve this problem, we need to start with an account of pure goal ascription.</div><div class="notes">An account of pure goal ascription is an account of how you could 
in principle infer facts about the goals to which actions are directed from
facts about joint displacements, bodily configurations and their effects 
(e.g. sounds).
Such an account is a computational theory of pure goal ascription.</div><div class="notes">Empahsize: in the case of speech, the problem is to get from acoustic and visual
information to articulatory gestures.</div><div class="notes">Empahsize: in the case of actions, the problem is to get from bodily configurations
and joint displacements to goals to which actions are directed.</div><p class="huge-glow-60">pure goal ascription</p><p style="margin-top:-2em;"><span class="infer">Infer</span><span> The Goals from The Evidence</span></p><div class="slide"><p class="em-above">The Goals: facts which goals particular actions are directed to...</p></div><div class="slide"><p class="em-above"> <span class="the-evidence">The Evidence</span><span>: facts about events and states of affairs that could be known without 
knowing which goals any particular actions are directed to, nor any facts
about particular mental states ...</span></p></div></div></div></div></div></section>
<section class="slide"><img src="/img/bkg_red.jpg" width="1024px" class="bkg"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">goal != intention</p></div></div></div></div></section><section class="slide"><img src="/img/ants1.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Some ants harvest plant hair and fungus in order to build traps to capture large insects; 
once captured, many worker ants sting the large insects, transport them and carve them up
\citep{Dejean:2005vb}.</div><div class="notes">We can think of the ants’ behaviour as goal-directed 
without also thinking of it as involving intention.</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><svg width="665px" height="445px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1"><defs><filter id="glow-large" width="3" height="3" x="-.5" y="-.5"><feGaussianBlur result="blurOut" in="SourceGraphic" stddeviation="15"></feGaussianBlur><feGaussianBlur result="blurOut2" in="SourceGraphic" stddeviation="10"></feGaussianBlur><feGaussianBlur result="blurOut3" in="SourceGraphic" stddeviation="5"></feGaussianBlur><femerge><femergenode in="blurOut"></femergenode><femergenode in="blurOut2"></femergenode><femergenode in="blurOut3"></femergenode><femergenode in="SourceGraphic"></femergenode></femerge></filter><filter id="glow-small" width="1.5" height="1.5" x="-.25" y="-.25"><feGaussianBlur result="blurOut" in="SourceGraphic" stddeviation="4"></feGaussianBlur><feGaussianBlur result="blurOut2" in="SourceGraphic" stddeviation="4"></feGaussianBlur><femerge><femergenode in="blurOut"></femergenode><femergenode in="blurOut2"></femergenode><femergenode in="SourceGraphic"></femergenode></femerge></filter><filter id="dropGlowBlack" width="1.5" height="1.5" x="-.25" y="-.25"><fegaussianblur in="SourceAlpha" stdDeviation="10" result="blur"></fegaussianblur><femerge><femergenode in="blur"></femergenode><femergenode in="SourceGraphic"></femergenode></femerge></filter><g id="action-ellipse"><ellipse cx="0" cy="0" rx="40" ry="40" fill="#000" stroke="#ffffff" stroke-width="3" pointer-events="none" filter="url(#dropGlowBlack)"></ellipse></g><g id="action-ellipse-neon"><ellipse cx="0" cy="0" rx="40" ry="40" fill="none" stroke="#A0A" stroke-width="3" pointer-events="none" filter="url(#glow-large)"></ellipse><ellipse cx="0" cy="0" rx="40" ry="40" fill="none" stroke="#2C75FF" stroke-width="3" pointer-events="none" filter="url(#glow-small)"></ellipse><ellipse cx="0" cy="0" rx="40" ry="40" fill="none" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse></g></defs><g transform="translate(0.5,0.5)"><g class="action-light"><ellipse cx="42" cy="42" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse><g transform="translate(12,27)"><switch><foreignobject pointer-events="all" width="59" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 59px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">light</font></div></div></foreignobject><text x="30" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="action-smoke"><ellipse cx="132" cy="402" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse><g transform="translate(91,387)"><switch><foreignobject pointer-events="all" width="81" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 81px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">smoke</font></div></div></foreignobject><text x="41" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="joint-action"><rect x="32" y="72" width="140" height="310" rx="21" ry="21" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none" filter="url(#dropGlowBlack)"></rect><g class="neon-rect hide"><rect x="32" y="72" width="140" height="310" rx="21" ry="21" fill="none" stroke="#A0A" stroke-width="3" pointer-events="none" filter="url(#glow-large)"></rect><rect x="32" y="72" width="140" height="310" rx="21" ry="21" fill="none" stroke="#2C75FF" stroke-width="3" pointer-events="none" filter="url(#glow-small)"></rect></g><rect x="32" y="72" width="140" height="310" rx="21" ry="21" fill="none" stroke="#ffffff" stroke-width="3" pointer-events="none"></rect><g class="action drop"><use xlink:href="#action-ellipse" x="87" y="132"></use><g class="neon-ellipse hide"><use xlink:href="#action-ellipse-neon" x="87" y="132"></use></g><g transform="translate(51,117)"><switch><foreignobject pointer-events="all" width="61" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 61px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">open</font></div></div></foreignobject><text x="31" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="action throw"><use xlink:href="#action-ellipse" x="82" y="242"></use><g class="neon-ellipse hide"><use xlink:href="#action-ellipse-neon" x="82" y="242"></use></g><g transform="translate(44,227)"><switch><foreignobject pointer-events="all" width="75" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 75px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">pour</font></div></div></foreignobject><text x="38" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="action discard"><use xlink:href="#action-ellipse" x="112" y="312"></use><g class="neon-ellipse hide"><use xlink:href="#action-ellipse-neon" x="112" y="312"></use></g><g transform="translate(68,297)"><switch><foreignobject pointer-events="all" width="88" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 88px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">tilt</font></div></div></foreignobject><text x="44" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g></g><g class="outcome amuse hide"><ellipse cx="562" cy="92" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse><g transform="translate(521,77)"><switch><foreignobject pointer-events="all" width="81" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 81px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">soak</font></div></div></foreignobject><text x="41" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="outcome scare hide"><ellipse cx="562" cy="272" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse><g transform="translate(527,257)"><switch><foreignobject pointer-events="all" width="69" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 69px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">scare</font></div></div></foreignobject><text x="35" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="outcome freakout hide"><ellipse cx="522" cy="362" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none"></ellipse><g transform="translate(483,332)"><switch><foreignobject pointer-events="all" width="77" height="62" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 75px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">freak out</font></div></div></foreignobject><text x="39" y="43" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="outcome block hide"><g class="block-glow hide"><ellipse cx="582" cy="182" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none" filter="url(#glow-large)" class="glow"></ellipse><ellipse cx="582" cy="182" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none" filter="url(#glow-large)" class="glow"></ellipse></g><ellipse cx="582" cy="182" rx="40" ry="40" fill="#000000" stroke="#ffffff" stroke-width="3" pointer-events="none" class="no-glow"></ellipse><g transform="translate(547,167)"><switch><foreignobject pointer-events="all" width="69" height="32" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; font-family: Lato; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 69px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">fill</font></div></div></foreignobject><text x="35" y="28" fill="#000000" text-anchor="middle" font-size="24px" font-family="Lato">[Not supported by viewer]</text></switch></g></g><g class="explain-collective-directedness hide"><g class="the-box"><rect x="242" y="262" width="210" height="60" rx="9" ry="9" fill="#ffffff" stroke="#ffffff" stroke-width="3" transform="rotate(-15,347,292)" pointer-events="none"></rect><g transform="translate(252,262)rotate(-15,94.5,30)"><switch><foreignobject pointer-events="all" width="189" height="62" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; width: 195px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;" class="invert">intention<span class="mr remove-me hide noinvert"> or motor representation</span><br/><span class="iasmr hide">or ???</span></div></div></foreignobject></switch></g></g><g class="coordinates hide"><path d="M 352 262 Q 332 212 307 192 Q 282 172 191.91 190.02" fill="none" stroke="#ffffff" stroke-width="3" stroke-miterlimit="10" pointer-events="none"></path><path d="M 185.29 191.34 L 193.23 185.16 L 191.91 190.02 L 195 193.99 Z" fill="#ffffff" stroke="#ffffff" stroke-width="3" stroke-miterlimit="10" pointer-events="none"></path><g transform="translate(199,155)"><switch><foreignobject pointer-events="all" width="126" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; overflow: hidden; max-height: 26px; max-width: 156px; width: 126px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">coordinates</font></div></div></foreignobject><text x="63" y="25" fill="#000000" text-anchor="middle" font-size="24px">[Not supported by viewer]</text></switch></g></g><g class="represents hide"><path d="M 352 262 Q 342 222 392 207 Q 442 192 531.95 183.01" fill="none" stroke="#ffffff" stroke-width="3" stroke-miterlimit="10" pointer-events="none" filter="url(#dropGlowBlack)"></path><path d="M 538.66 182.33 L 530.16 187.71 L 531.95 183.01 L 529.26 178.75 Z" fill="#ffffff" stroke="#ffffff" stroke-width="3" stroke-miterlimit="10" pointer-events="none" filter="url(#dropGlowBlack)"></path><g transform="translate(374,167)rotate(-12,58,12)"><switch><foreignobject pointer-events="all" width="116" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 24px; color: rgb(0, 0, 0); line-height: 1.26; vertical-align: top; overflow: hidden; max-height: 26px; max-width: 156px; width: 116px; white-space: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font color="#ffffff">specifies</font></div></div></foreignobject><text x="58" y="25" fill="#000000" text-anchor="middle" font-size="24px">[Not supported by viewer]</text></switch></g></g></g></g></svg><div class="notes">As this illustrates, 
some actions involving are purposive in the sense that</div><div class="slide"><div data-what=".outcome" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">among all their actual and possible consequences,</div></div><div class="slide"><div data-what=".outcome.block .block-glow" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">there are outcomes to which they are directed</div></div><div class="slide"><div data-what=".joint-action .neon-rect" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">In such cases we can say that the actions are clearly purposive.</div></div><div class="slide"><div data-what=".explain-collective-directedness" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">The standard answer to this question involves intention.</div></div><div class="slide"><div data-what=".represents" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">An intention (1) specifies an outcome,</div></div><div class="slide"><div data-what=".coordinates" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">(2) coordinates the one or several activities which comprise the action;</div><div class="notes">and (3) coordinate these activities in a way that would normally facilitate the outcome’s occurrence.</div><div class="notes">What binds particular component actions together into larger purposive actions?  
It is the fact that these actions are all parts of plans involving a single intention.
What singles out an actual or possible outcome as one to which the component 
actions are collectively directed?  It is the fact that this outcome is 
represented by the intention.</div><div class="notes">So the intention is what binds component actions together into purposive actions and 
links the action taken as a whole to the outcomes to which they are directed.</div></div></div></div></div></section><section class="slide"><img src="/img/goal_ascription_requirements/Slide1.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">It is important to see that the third item---representing the directedness---is necessary.</div><div class="notes">This is quite simple but very important, so let me slowly explain why goal ascription requires representing the directedness of an action to an outcome.</div></div></div></div></section><section class="slide"><img src="/img/goal_ascription_requirements/Slide2.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Imagine two people, Ayesha and Beatrice, who each intend to break an egg.
Acting on her intention, Ayesha breaks her egg.
But Beatrice accidentally drops her egg while carrying it to the kitchen.
So Ayesha and Beatrice perform visually similar actions which result in the same type of outcome,
the breaking of an egg; but Beatrice's action is not directed to the outcome of her action whereas
Ayesha's is.</div></div></div></div></section><section class="slide"><img src="/img/goal_ascription_requirements/Slide3.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">Goal ascription requires the ability to distinguish between Ayesha's action and Beatrice's action. 
This requires representing not only actions and outcomes but also the directedness of actions to outcomes.</div><div class="notes">This is why I say that goal ascription requires representing the directedness of an action to an outcome, and not just representing the action and the outcome.</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p>Requirements:</p><p class="hem-around">(1) reliably: R(a,G)  <span class="when">when</span><span> and </span><span class="only-when">only when</span><span> a is directed to G</span></p><p class="hem-around readily-detectable">(2) R(a,G) is  readily detectable</p><p class="hem-around detectable-without">(3) R(a,G) is readily detectable without any knowledge of mental states</p><p class="hem-around">&nbsp;</p><p class="hem-around candidate-teleological hide">R(a,G) =df a causes G?</p><p class="hem-around candidate-causes hide">R(a,G) =df a causes G?</p><p class="hem-around candidate-ts hide">R(a,G) =df a ‘is seen as the most justifiable action towards [G] that is available within the constraints of reality’?</p><div class="slide"><div data-what=".candidate-teleological" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">Why not define $R$ in terms of teleological function?
This would enable us to meet the first condition but not the second.
How could we tell whether an action happens because it brought about a particular outcome in the past? 
This might be done with insects.
But it can's so easily be done with primates, who have a much broader repertoire of actions.</div><div class="slide"><div data-what=".detectable-without" data-cls="bkg-grey-row " class="dv dv-addclass"></div></div><div class="slide"><div data-what=".detectable-without" data-cls="bkg-grey-row " class="dv dv-removeclass"></div><div data-what=".readily-detectable" data-cls="bkg-red-row " class="dv dv-addclass"></div></div><div class="slide"><div data-what=".readily-detectable" data-cls="bkg-red-row " class="dv dv-removeclass"></div><div data-what=".candidate-teleological" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}" class="dv dv-velocity"></div></div></div><div class="slide"><div data-what=".candidate-causes" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">How about taking $R$ to be causation?
That is, how about defining $R(a,G)$ as $a$ causes $G$?
This proposal does not meet the first criterion, (1), above.
We can see this by mentioning two problems.

[*Might skip over-generate and discuss that as a problem for Rationality/Efficiency]
First problem: actions typically have side-effects which are not goals.
For example,
%---not a good example because can't be avoided by any account
%--- (would require attribution of desire)
%For example, walking to the corner results in me warming up, in me expending energy, and in me being at the corner.
%Sometimes I walk to the corner for exercise,
%so that being at the corner is an unwanted side-effect (I then have to walk back).
%And sometimes I walk to the corner to be at the corner (so that expending energy is an unwanted side-effect, I'd rather have been chauffeured there).  
suppose that I walk over here with the goal of being next to you.
This action has lots of side-effects: 
\begin{itemize}
\item 	I will be at this location.
\item	I will expend some energy.
\item	I will be further away from the front
\end{itemize}
These are all causal consequence of my action.
But they are not goals to which my action is directed.
So this version of $R$ will massively over-generate goals.

Second problem: actions can fail.  [...]
So this version of $R$ will under-generate goals.</div><div class="slide"><div data-what=".only-when" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".only-when" data-cls="bkg-red" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".only-when" data-cls="bkg-red" class="dv dv-removeclass"></div><div data-what=".when" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".when" data-cls="bkg-red" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".when" data-cls="bkg-red" class="dv dv-removeclass"></div><div data-what=".candidate-causes" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}" class="dv dv-velocity"></div></div></div><div class="slide"><div data-what=".candidate-ts" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_3"><p style="border-right:1px grey dashed;padding-right:9px;margin-right:-9px" class="quote notes handout show"><span>‘an action can be explained by a </span><span class="goal-state">goal state</span><span> if, and only if, it is </span><span class="seen-as">seen as </span><span> the </span><span class="most-justifiable"> <span class="most">most</span><span> justifiable action </span></span><span>towards that </span><span class="goal-state">goal state</span><span> that is available within the constraints of reality’</span></p><div class="notes handout ctd">\citep[p.~255]{Csibra:1998cx}</div><p class="quote right grey-text">Csibra & Gergely (1998, 255)</p></div><div class="slide"><div data-what=".goal-state" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".goal-state" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">A goal is an outcome to which an action is directed.
A goal-state is a representation of the outcome in virtue of which
the action is directed to that outcome.
So an intention is a goal state.
By contrast, a goal is not a mental state at all.
In order for this to be about *pure* goal ascription, we need to ignore
the Csibra and Gergely’s odd choice of terminology.</div></div><div class="slide"><div data-what=".goal-state" data-cls="bkg-invert" class="dv dv-removeclass"></div></div><div class="slide"><div data-what=".most-justifiable" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".most-justifiable" data-cls="bkg-pink" class="dv dv-addclass"></div></div><div class="slide"><div class="grid_9"><p class="step1 hide">1. action a is directed to some goal;</p><p class="step2 hide"> 2. actions of a’s type are <span class="normally">normally</span><span> means of realising outcomes of G’s type;</span></p><p class="step3 hide">3. no available alternative action is a significantly  <span class="better">better*</span><span> means of realising outcome G;</span></p><p class="step4 hide">4. the occurrence of outcome G is  <span class="desirable">desirable</span><span>;</span></p><p class="step5 hide">5. there is no other outcome, G′, the occurrence of which would be at least comparably desirable and where (2) and (3) both hold of G′ and a</p><p class="step6 hide">Therefore:</p><p class="step7">6. <span class="g">G</span><span>  is a goal to which action </span><span class="a">a</span><span>  is directed.</span></p></div><div class="slide"><div data-what=".step7 .g" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".step7 .g" data-cls="bkg-invert" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".step7 .g" data-cls="bkg-invert" class="dv dv-removeclass"></div><div data-what=".step7 .a" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".step7 .a" data-cls="bkg-invert" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".step7 .a" data-cls="bkg-invert" class="dv dv-removeclass"></div></div><div class="slide"><div data-what=".step1" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">We start with the assumption that we know the event is an action.</div></div><div class="slide"><div data-what=".step2" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><div data-what=".most-justifiable" data-cls="bkg-pink" class="dv dv-removeclass"></div><div data-what=".normally" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".normally" data-cls="bkg-invert" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".seen-as" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".seen-as" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">Why normally? Because of the ‘seen as’.</div></div><div class="slide"><div data-what=".normally" data-cls="bkg-invert" class="dv dv-removeclass"></div><div data-what=".seen-as" data-cls="bkg-invert" class="dv dv-removeclass"></div></div><div class="slide"><div data-what=".most" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".most" data-cls="bkg-invert" class="dv dv-addclass"></div></div><div class="slide"><div data-what=".step3" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><div data-what=".most" data-cls="bkg-invert" class="dv dv-removeclass"></div></div><div class="slide"><div data-what=".better" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".better" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">What does it mean to say that one means is better than another?
There are different respects in which one action can be better than another as a means 
to some realising some outcome; for example, one action can require less effort than 
another, or one action be a more reliable way to bring the outcome about than another.</div><div class="notes handout">An action of type $a'$ is a \emph{better} means of realising outcome $G$ in a given situation than an action of type $a$ if, for instance, actions of type $a'$ normally involve less effort than actions of type $a$ 
in situations with the salient features of this situation 
and everything else is equal; 
or if, for example, actions of type $a'$ are normally more likely to realise outcome $G$ than actions of type $a$
in situations with the salient features of this situation 
and everything else is equal.</div></div><div class="slide"><div data-what=".better" data-cls="bkg-invert" class="dv dv-removeclass"></div></div><div class="slide"><div data-what=".quote" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}" class="dv dv-velocity"></div><div data-what=".step6" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">Any objections?</div><div class="notes">I have an objection.
Consider a case in which I perform an action directed to 
the outcome of pouring some hot tea into a mug.
Could this pattern of inference imply that the outcome be the goal of my action?
Only if it also implies that moving my elbow is a goal of my action
as well.
And pouring some liquid. 
And moving air in a certain way.
And ...</div><div class="notes">How can we avoid this objection?</div></div><div class="slide"><div data-what=".step4" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><div data-what=".desirable" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".desirable" data-cls="bkg-pink" class="dv dv-addclass"></div><div class="notes">Doesn’t this conflict with the aim of explaining *pure* behaviour reading?
Not if desirable is understood as something objective.
[explain]</div></div><div class="slide"><div data-what=".desirable" data-cls="bkg-pink" class="dv dv-removeclass"></div><div class="notes">Now we are almost done, I think.</div></div><div class="slide"><div data-what=".step5" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">We just need to add a clause ensuring that the goal in question is maximally 
desirable; this is an attempt to reduce overgeneration of goals.</div></div><div class="slide"><div data-what=".quote" data-css="{&quot;blur&quot;:0}" data-options="{&quot;duration&quot;:400}" class="dv dv-velocity"></div><div class="notes">OK, I think this is reasonably true to the quote.
So we’ve understood the claim.
But is it true?</div></div><div class="slide"><div data-what=".quote" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}" class="dv dv-velocity"></div><div data-what=".better" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".better" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">How good is the agent at optimising the selection of means to her goals?
And how good is the observer at identifying the optimality of means in relation to outcomes?
\textbf{
For optimally correct goal ascription, we want there to be a match between
(i) how well the agent can optimise her choice of means
and
(i) how well the observer can detect such optimality.}
Failing such a match, the inference will not result in correct goal ascription.</div><div class="notes">But I don’t think this is an objection to the Teleological Stance as a
computational theory of pure goal ascription.  It is rather a detail
which concerns the next level, the level of representations and algorithms.
The computational theory imposes demands at the next level.</div><div class="handout">‘Such calculations require detailed knowledge of biomechanical factors that
determine the motion capabilities and energy expenditure of agents. However,
in the absence of such knowledge, one can appeal to heuristics that approximate
the results of these calculations on the basis of knowledge in other domains
that is certainly available to young infants. For example, the length of
pathways can be assessed by geometrical calculations, taking also into
account some physical factors (like the impenetrability of solid objects).
Similarly, the fewer steps an action sequence takes, the less effort it might
require, and so infants’ numerical competence can also contribute to efficiency
evaluation.’ \citep{csibra:2013_teleological}</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p>Requirements:</p><p class="hem-around">(1) reliably: R(a,G)  <span class="when">when</span><span> and </span><span class="only-when">only when</span><span> a is directed to G</span></p><p class="hem-around readily-detectable">(2) R(a,G) is  readily detectable</p><p class="hem-around detectable-without">(3) R(a,G) is readily detectable without any knowledge of mental states</p><p class="hem-around">&nbsp;</p><p class="hem-around candidate-teleological hide">R(a,G) =df a causes G?</p><p class="hem-around candidate-causes hide">R(a,G) =df a causes G?</p><p class="hem-around candidate-ts hide">R(a,G) =df a ‘is seen as the <span class="most-justifiable">most justifiable</span><span> action towards [G] that is available within the constraints of reality’?</span></p><div data-what=".candidate-teleological" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:0}" class="dv dv-velocity"></div><div data-what=".candidate-teleological" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:0}" class="dv dv-velocity"></div><div data-what=".candidate-causes" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:0}" class="dv dv-velocity"></div><div data-what=".candidate-causes" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:0}" class="dv dv-velocity"></div><div data-what=".candidate-ts" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:0}" class="dv dv-velocity"></div><div class="slide"><div data-what=".most-justifiable" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".most-justifiable" data-cls="bkg-invert" class="dv dv-addclass"></div><div class="notes">It will work if we can match observer and agent: both must be ‘equally optimal’.
But how can we ensure this?</div></div><div class="slide"><div data-what=".only-when" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".only-when" data-cls="bkg-orange" class="dv dv-addclass"></div><div data-what=".when" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".when" data-cls="bkg-orange" class="dv dv-addclass"></div><div class="notes">How good is the agent at optimising the rationality, or the efficiency, of her actions?
And how good is the observer at identifying the optimality of actions in relation to outcomes?
\textbf{
If there are too many discrepancies between
		how well the agent can optimise her actions
	and
		how well the observer can detect optimality,
then these principles will fail to be sufficiently reliable}.</div></div></div></div></div></div></section><section id="motor_theory_goal_ascription" class="slide"><img src="/img/bkg/joint_action01/AAaaDSC_AA_4400.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="position:relative; top:425px" class="title-block"><div class="title-container"><h2 class="title1">The Motor Theory of Goal Ascription</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{The Motor Theory of Goal Ascription}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{The Motor Theory of Goal Ascription}</div><div class="notes">  </div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center"><span class="representational">representational</span><span>&nbsp; </span><span class="functional hide"> functional </span><span> goal ascription</span></p><div class="notes">Two forms of goal ascription, representational and functional (\citealp{gallese:2011_what}).
In \emph{representational goal ascription}, three things must be represented: an action, an outcome and the relation between this outcome and the action in virtue of which the outcome is a goal of the action.
% jacob:2012_sharing: ‘Ascribing a goal to an agent consists in forming a belief (or judgment) about an agent that he or she has a goal or is performing some goal-directed action.’
In \emph{functional goal ascription}, the relation between action and outcome is captured without being represented.
To say that this relation is \emph{captured} is to say that there is a process which ensures that the outcome represented is a goal of the action.</div><div class="slide"><div data-what=".representational" data-css="{&quot;text-decoration&quot;:&quot;line-through&quot;}" class="dv dv-style"></div><div class="notes">Motor representations cannot suffice for representational goal ascription. 
It is true that, in someone observing an action there can be motor representations of outcomes which, non accidentally, are the goals of the observed action.
But this is not enough.
There would have to be, in addition, a motor representation of an intention, or of a motor representation or of some other goal-state, or of a function. 
But there are no such motor representations.</div></div><div class="slide"><div data-what=".functional" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p>Own action:</p><p>Why are outcomes you represent motorically often among the goals of your actions?</p><div class="notes">How does it ever come about that an outcome represented motorically in observing an action is an outcome to which that action is directed?
First consider a parallel question about performing rather than observing actions.
Suppose you are alone and not observing or imagining any other actions.
When performing actions in this situation, outcomes represented motorically in you will normally be
among the goals of your actions; that is, they will be outcomes to which your actions are directed.
What ensures this correspondence between outcomes represented and goals?</div><div class="slide"><p>Because planning-like processes.</p><div class="notes">It is the role of the
representation in controlling how the action unfolds. Representations of outcomes trigger
planning-like motor processes whose function is to cause actions that will bring about the outcomes
represented \citep{miall:1996_forward,arbib:1985_coordinated,rosenbaum:2012_cognition}.</div></div><div class="slide"><p class="em-above">Another’s action</p><p>Why are outcomes you represent motorically often among the goals of her actions?</p><div class="notes">Now return to
observing rather than perform actions. What ensures the correspondence between outcomes represented
motorically and goals when you are merely observing another act?</div></div><div class="slide"><p>Because planning-like processes.</p><div class="notes">The answer, we suggest, is roughly that planning-like processes can be used not only to control
actions but also to predict them.
Let us explain.</div></div></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/sinigaglia_butterfill_2005_fig1.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Sinigalia & Butterfill 2015, figure 1</p><div class="notes">There is evidence that a motor representation of an outcome can cause a determination of which movements are likely to be performed to achieve that outcome \citep[see, for instance,][]{kilner:2004_motor, urgesi:2010_simulating}. Further, the processes involved in determining how observed actions are likely to unfold given their outcomes are closely related, or identical, to processes involved in performing actions. 
This is known in part thanks to studies of how observing actions can facilitate performing actions congruent with those observed, and can interfere with performing incongruent actions \citep{
	brass:2000_compatibility, 
	craighero:2002_hand, 
	kilner:2003_interference, 
	costantini:2012_does}. 
Planning-like processes in action observation have also been demonstrated by measuring observers' predictive gaze.  If you were to observe just the early phases of a grasping movement, your eyes might jump to its likely target, ignoring nearby objects \citep{ambrosini:2011_grasping}. These proactive eye movements resemble those you would typically make if you were acting yourself \citep{Flanagan:2003lm}. 
Importantly, the occurrence of such proactive eye movements in action observation depends on your
representing the outcome of an action motorically; even temporary interference in the observer's
motor abilities will interfere with the eye movements \citep{Costantini:2012fk}.
These proactive eye movements also depend on planning-like processes; requiring the observer to
perform actions incongruent with those she is observing can eliminate proactive eye movements
\citep{Costantini:2012uq}. This, then, is further evidence for planning-like motor processes in
action observation.</div><div class="notes">So observers represent outcomes motorically and these representations trigger planning-like processes
which generate expectations about how the observed actions will unfold and their sensory consequences.
Now the mere occurrence of these processes is not sufficient to explain why, in action observation,
an outcome represented motorically is likely to be an outcome to which the observed action is
directed.</div><div class="notes">To take a tiny step further, we conjecture that, in action observation, \textbf{motor representations of
outcomes are weakened to the extent that the expectations they generate are unmet}
\citep[compare][]{Fogassi:2005nf}.
A motor representation of an outcome to which an observed action is not directed is likely to
generate incorrect expectations about how this action will unfold, and failures of these
expectations to be met will weaken the representation.
This is what ensures that there is a correspondence between outcomes represented motorically in
observing actions and the goals of those actions.</div></div></div></div></section><section id="marrs_levels" class="slide"><img src="/img/bkg/joint_action01/DSC_AB_6649.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="position:relative; top:425px" class="title-block"><div class="title-container"><h2 class="title1">Marr’s Threefold Distinction</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{Marr’s Threefold Distinction}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{Marr’s Threefold Distinction}</div><div class="notes">Marr helpfully distinguishes computational description (What is the thing for and how does it achieve this?)
from representations and algorithms (How are the inputs and outputs represented, and how is the transformation accomplished?)
and from the hardware implementation (How are the  representations and algorithms physically realised?)</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="handout">\citet[p.~22ff]{Marr:1982kx} distinguishes:</div><div class="handout">\begin{itemize}</div><div class="handout">\item computational description---What is the thing for and how does it achieve this?</div><div class="handout">\item representations and algorithms---How are the inputs and outputs represented, and how is the transformation accomplished?</div><div class="handout">\item hardware implementation---How are the representations and algorithms physically realised?</div><div class="handout">\end{itemize}</div><div class="notes">One possibility is to appeal to David Marr’s famous three-fold distinction
bweteen levels of description of a system: the computational theory, the 
representations and algorithm, and the hardware implementation.</div><div class="notes">This is easy to understand in simple cases.
To illustrate, consider a GPS locator.
It receives information from four satellites and tells you where on Earth the device is.</div><div class="notes">There are three ways in which we can characterise this device.</div><div class="slide"><div class="computational"><p>1. computational description</p><div class="notes">First, we can explain how in theory it is possible to infer the 
device’s location from it receives from satellites.
This involves a bit of maths: given time signals from four different satellites,
you can work out what time it is and how far you are away from each
of the satellites.
Then, if you know where the satellites are and what shape the Earth is,
you can work out where on Earth you are.</div><div class="slide"><p class="indent">-- What is the thing for and how does it achieve this?</p></div><div class="notes">The computational description tells us what the GPS locator does and 
what it is for.
It also establishes the theoretical possibility of a GPS locator.</div><div class="notes">But merely having the computational description does not enable you to build 
a GPS locator, nor to understand how a particular GPS locator works.
For that you also need to identify representations and alogrithms ...</div></div></div><div class="slide"><p class="em-above">2. representations and algorithms</p><div class="notes">At the level of representations and algorthms we specify 
how the GPS receiver represents the information it receives from the satellites
(for example, it might in principle be a number, a vector or a time).
We also specify the algorithm the device uses to compute the time and its location.
The algorithm will be different from the computational theory: it is a procedure
for discovering time and location.
The algorithm may involve all kinds of shortcuts and approximations.
And, unlike the computational theory, constraints on time, memory and other
limited resources will be evident.</div><div class="slide"><div class="notes">So an account of the representations and algorithms tells us ...</div><p class="notes show indent">-- How are the inputs and outputs represented, and how is the transformation accomplished?</p></div></div><div class="slide"><p class="em-above">3. hardware implementation</p><div class="notes">The final thing we need to understand the GPS locator is a description of the
hardware in which the algorithm is implemented.  It’s only here that 
we discover whether the device is narrowly mechanical device, using cogs, say, 
or an electronic device, or some new kind of biological entity.</div></div><div class="slide"><p class="indent">-- How are the  representations and algorithms physically realised?</p><div class="notes">The hardware implementation tells us how the representations and algorithms are represented physically.</div></div><p class="right grey-text">Marr (1992, 22ff)</p><div class="slide"><div data-what="p" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}" class="dv dv-velocity"></div><div class="notes">How is this relevant to the teleological stance?
It provides a computational description of goal ascription.
Whereas the Motor Theory provides an account of the representations and algorithms</div></div><div class="slide"><div data-what="p" data-css="{&quot;blur&quot;:0}" data-options="{&quot;duration&quot;:400}" class="dv dv-velocity"></div><div data-what=".computational" data-cls="bkg-grey-row " class="dv dv-addclass"></div><div class="notes">I suggest that an account of radical interpretation is suppsoed to provide
a computational description of social cognition; it tells us what social
cognition is for and how, in the most abstract sense, it is possible.</div></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">The Motor Theory of Goal Ascription </p><p class="center">is</p><p class="center">the Teleological Stance</p></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center huge-glow-180">How?</p></div></div></div></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">Motor representations concerning the goals of observed actions sometimes facilitate the identification of goals.</p></div></div></div></div><div class="notes">Now we’ve solved this: the Motor Theory of Goal Ascription is the solution.</div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">Also recall the question</div><p class="notes show">How could the objects of categorical perception of speech be articulatory gestures?</p><div class="notes">The puzzle here is simple.
Categorical perception of speech happens raidly, and goal-directed actions
are complex.  How can something so complex be computed so quickly?</div><div class="notes">Put it another way: how is it that the articulatory gesture is identified?</div><p class="em-above notes handout show">‘Humans [can] understand speech delivered at a rate of 20 to 30 ... phonemes per second’</p><div class="notes handout ctd"> \citep{Devlin:2006qg}</div><p class="right grey-text">Devlin (2006)</p><div class="notes">We’ve also solved this with the Motor Theory of Goal Ascription.
(a) Motor processes are fast enough (production speed = comprehension speed);
(b) The Motor Theory explains how motor processes can implement functional 
goal ascription, which is to say, how they can get from observed bodily configurations
and joint displacements to the identification of goals to which the action is directed.</div></div></div></div></div></section><section id="puzzle_thought_experience_motoric" class="slide"><img src="/img/bkg/joint_action01/DSC_AB_6862.low.JPG" class="bkg"/><div class="spacer">&nbsp;</div><div style="position:relative; top:425px" class="title-block"><div class="title-container"><h2 class="title1">A Puzzle about Thought, Experience and the Motoric</h2></div></div></section><div class="handout">&nbsp;</div><div class="handout">\section{A Puzzle about Thought, Experience and the Motoric}</div><div class="notes notes-header-tex">&nbsp;</div><div class="notes notes-header-tex">\section{A Puzzle about Thought, Experience and the Motoric}</div><div class="notes">Motor representations occur when merely observing others act and sometimes influence thoughts about the
goals of observed actions. Further, these influences are content-respecting: what you think about an
action sometimes depends in part on how that action is represented motorically in you. The existence of
such content-respecting influences is puzzling. After all, motor representations do not feature
alongside beliefs or intentions in reasoning about action; indeed, thoughts are inferentially isolated
from motor representations. So how could motor representations have content-respecting influences on
thoughts?</div>
<!-- param @cls and param @options should be objects-->



<section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="hem-around">In action observation, motor representations of outcomes <span class="step2 hide">...</span></p><p class="hem-around step2 hide">... underpin functional goal ascription<span class="step3 hide">, and</span></p><p class="hem-around step3 hide">sometimes facilitate the identification of goals in thought.</p><div class="slide"><div data-what=".step2" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><div data-what=".step3" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><p class="em-above">So</p><p>where motor representations influence a thought about an action being directed to a particular outcome, there is normally a motor representation of this outcome, or of a matching outcome.</p><div class="notes">This conclusion entails that motor representations have content-respecting
influences on thoughts. It is the fact that one outcome rather than another is represented
motorically which explains, at least in part, why the observer takes this outcome (or a matching
one) to be an outcome to which the observed action is directed.</div></div><div class="slide"><p class="em-above">But</p><p>how could motor representations have content-respecting influences on thoughts <span class="step4 hide">given their inferential isolation</span><span>?</span></p><div class="notes">But how could motor representations
have content-respecting influences on thoughts? One familiar way to explain content-respecting
influences is to appeal to inferential relations. To illustrate, it is no mystery that your beliefs
have content-respecting influences on your intentions, for the two are connected by processes of
practical reasoning. But motor representation, unlike belief and intention, does not feature in
practical reasoning. Indeed, thought is inferentially isolated from it. How then could any motor
representations have content-respecting influences on thoughts?</div><div class="slide"><div data-what=".step4" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="q hide"><p class="center huge-glow-180 hem-above">?</p></div></div></div></div></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">motor representation ->  <span class="experience">experience of action</span><span>  -> thought</span></p><div class="notes">In something like the way experience may tie thoughts about seen objects to the representations
involved in visual processes, so also it is experience that connects what is represented
motorically to the objects of thought.</div><div class="notes">[significance]
This may matter for understanding thought about action. On the face of it, the inferential
isolation of thought from motor representation makes it reasonable to assume that an account of
how humans think about actions would not depend on facts about motor representation at all. But
the discovery that motor representations sometimes shape experiences revelatory of action
justifies reconsidering this assumption. It is plausible that people sometimes have reasons for
thoughts about actions, their own or others', that they would not have if it were not for their
abilities to represent these actions motorically. To go beyond what we have considered here, it
may even turn out that an ability to think about certain types of actions depends on an ability
to represent them motorically.</div><div class="notes">[consequence]
One consequence of our proposal concerns how experiences of one's own actions relate to
experiences of others' actions. For almost any action, performing it would typically involve
receiving perceptual information quite different to that involved in observing it. This may
suggest that experiences involved in performing a particular action need have nothing in common
with experiences involved in observing that action. However, two facts about motor
representation, its double life and the way it shapes experience, jointly indicate otherwise. For
actions directed to those goals that can be revealed by experiences shaped by motor
representations, there are plausibly aspects of phenomenal character common to experiences
revelatory of one's own and of others' actions. In some respects, what you experience when others
act is what you experience when you yourself act.</div></div></div></div></div><div class="slide"><div data-what=".q" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}" class="dv dv-velocity"></div></div><div class="slide"><div data-what=".experience" data-cls="transition-04" class="dv dv-addclass"></div><div data-what=".experience" data-cls="bkg-red" class="dv dv-addclass"></div><div class="notes">The claim that there is expeirence of action is based on an earlier argument.
I now want to review and then object to that argument.
(The conclusion may be correct, but the argument does not establish it.)</div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">What do we experience when we encounter others’ <span class="speech">(speech)</span><span> actions?</span></p><p class="em-above center">Indirect Hypothesis <span class="direct-h">vs Direct Hypothesis</span></p></div></div></div></div></section><section class="slide"><img src="/img/categorical_perception_of_speech/Slide10.jpg" width="1024px" class="bkg"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">The difference in differences ...</div><div class="notes">Here is the argument. Consider two sequences of sensory encounters: (a) a sequence of sensory
encounters with two phonetic events that do not differ with respect to category (both are
realisations of /d/, say), and (b) a sequence of sensory encounters with two phonetic events that do
so differ (one is a realisation of /d/ the other of /g/, say).11 Let the events encountered in the
first sequence differ from each other acoustically in the same way and by the same amount as the
events encountered in the second sequence differ from each other. (That it is possible to find two
such pairs of events follows from the fact that we enjoy categorical perception of speech.) The two
sequences are depicted in Fig. 3. Now:</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><p class="hem-around">(1) The second sequence of sensory encounters, (b), differ from each other more in phenomenal character than the first sequence of sensory encounters, (a), differ from each other.</p><p class="hem-around slide">(2) This difference in differences in phenomenal character is a fact in need of explanation.</p><div class="slide"><p class="hem-around claim3">(3) The difference cannot be fully explained by appeal only to perceptual experiences as of acoustic features of the stimuli.</p></div><p class="hem-around slide">(4) The difference can be explained in terms of perceptual experiences as of phonetic properties.</p><p class="hem-around slide">(5) There is no better explanation of the difference.</p><div data-what=".claim3" data-attr="style" data-value="z-index:0" class="dv dv-attr"></div><div class="slide"><div data-what=".claim3" data-cls="bkg-red-row " class="dv dv-addclass"></div><div class="notes">This is less obvious than it might seem.
We infer it from the facts about relevant acoustic features of the stimuli, and the
way acoustic processes work.
But we know that motor processes are also at work.
And we can’t assume that motor processes do not influence acoustic processes.
In fact we know that motor processes affect acoustic expeirences.</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">motor processes affect acoustic expeirences</p></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/repp_knoblich_2009_fig3.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source">Repp & Knoblich 2009, figure 3</p><div class="notes">Repp and Knoblich (\citeyear{repp:2007_action}) asked expert and non-expert pianists to press two keys in sequence, where the first key was sometimes to the left, and sometimes to the right, of the second key.  The key presses produced an \emph{ambiguous tone pair}, that is, a pair with the property that the first tone is sometimes perceived as lower in pitch than the second whereas at other times it is perceived  as higher in pitch \citep{deutsch:1987_tritone}.   
The tones always occurred in the same order regardless of which key was pressed first.  By asking subjects to report how they perceived the relative pitches of the tones, Repp and Knoblich found that, for the expert pianists, the direction of the key presses influenced the perceived direction of the change in pitch. Could what influences experience in this case be not a motor representation but merely the occurrence of a movement, or perhaps even the perception of a movement of the subject's own fingers? Against these possibilities, note that the effect was not observed in non-expert pianists: for them the direction of movement did not measurably influence the perceived pitches. Since the direction of movement was the same for both groups, if the influence were due to movement only we would expect it to occur irrespective of piano-playing expertise. Instead it seems likely that differences in expertise between the two groups of subjects affected how the movements they performed were represented motorically, and that these differences in motor representation are in turn what explains their perceptions of relative pitches.</div><div class="notes">It is not only in performing action that motor representation can influence experience: the same can occur in observing action. Thus in another experiment, Repp and Knoblich (\citeyear{repp:2009_performed}) compared observing someone else perform a sequence of key presses with performing the same sequence oneself.  They found the same effect on experiences of an ambiguous tone pair in expert pianists regardless of whether they were observing or performing the action.  
Sometimes, which judgement about pitch an experience provides a reason for depends on what is represented motorically; and this dependence is systematic, of course, for it reflects how pianos work.
These studies, and others like them,%
\footnote{
See also \citet{zwickel:2010_interference} who investigate effects of action on visual experience of motion, and \citet{schutz-bosbach:2007_perceptual} for a review.
}
provide relatively direct evidence that motor representation can shape experience.</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">motor processes affect acoustic expeirences</p></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><p class="hem-around">(1) The second sequence of sensory encounters, (b), differ from each other more in phenomenal character than the first sequence of sensory encounters, (a), differ from each other.</p><p class="hem-around">(2) This difference in differences in phenomenal character is a fact in need of explanation.</p><p class="hem-around claim3">(3) The difference cannot be fully explained by appeal only to perceptual experiences as of acoustic features of the stimuli.</p><p class="hem-around">(4) The difference can be explained in terms of perceptual experiences as of phonetic properties.</p><p class="hem-around">(5) There is no better explanation of the difference.</p><div data-what=".claim3" data-cls="bkg-red-row " class="dv dv-addclass"></div><div data-what=".claim3" data-attr="style" data-value="z-index:0" class="dv dv-attr"></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">What do we experience when we encounter others’ <span class="speech">(speech)</span><span> actions?</span></p><p class="em-above center">Indirect Hypothesis <span class="direct-h">vs Direct Hypothesis</span></p></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="huge-glow">Puzzle</p><p style="margin-top:-1.2em">How could motor representations have content-respecting influences on thoughts given their inferential isolation?</p></div></div></div></div></section></div></body></html>